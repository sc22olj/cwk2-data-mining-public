{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code which combines ideas from the previous prototypes, and uses a model trained on manually tagged data rather than a generic spaCy/scispaCy model. Includes scispacy entity linker and abbreviation detector\n",
    "\n",
    "Using a premade python script to preprocess the n2c2 files. Preprocessing script is taken from weasel tutorials, see https://github.com/explosion/projects/blob/v3/tutorials/ner_pytorch_medical/scripts/preprocess.py\n",
    "\n",
    "- Use preprocess file from weasel, however do the training manually using spacy commands rather than wrapper commands\n",
    "- Ignore pytorch stuff and use a more simple model instead\n",
    "- Follow spacy tutorial for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example uses:\n",
    "- Extract all the medications that a patient is taking\n",
    "- Extract all the symptoms that a patient is experiencing\n",
    "- Extract all the diseases that a patient has\n",
    "- Extract all the procedures that a patient has undergone\n",
    "\n",
    "- Perform sentiment analysis on a patient's EHR to give an overall idea of the patient's health\n",
    "\n",
    "- Allow filtering of the data based on the disease that they are related to or the date that they were recorded\n",
    "\n",
    "\n",
    "Conforming to the existing tags in the n2c2 dataset:\n",
    "\n",
    "- Extract all the people that are mentioned in the document\n",
    "- Extract all the problems that are mentioned in the document\n",
    "- Extract all the tests that are mentioned in the document\n",
    "- Extract all the treatments that are mentioned in the document\n",
    "\n",
    "- Perform sentiment analysis on a patient's EHR to give an overall idea of the patient's health\n",
    "'''\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.tokens import Doc\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "from scispacy.linking import EntityLinker\n",
    "import numpy as np\n",
    "\n",
    "# Uses textblob from NLTK\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Load the newly trained model\n",
    "nlp = spacy.load(\"output/model-best/\")\n",
    "\n",
    "# Sentencizer for similarity scores\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Textblob for sentiment analysis\n",
    "nlp.add_pipe(\"spacytextblob\")\n",
    "\n",
    "# Add abbreviation pipe and entity linker to the pipeline\n",
    "nlp.add_pipe(\"abbreviation_detector\")\n",
    "\n",
    "# Take user input for the file name\n",
    "# Set up for parsing XML from n2c2\n",
    "path = input(\"Enter the path to the file to process: \")\n",
    "tree = ET.parse(path)\n",
    "root = tree.getroot()\n",
    "text = root.find('TEXT').text\n",
    "\n",
    "# Process the given EHR\n",
    "doc = nlp(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform NER\n",
    "def extract_entities(doc, entity):\n",
    "\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == entity:\n",
    "            entities.append(ent.text)\n",
    "\n",
    "    print(\"\\n\" + entity.capitalize() + \"(s): \\n\")\n",
    "\n",
    "    # Sort entities alphabetically\n",
    "    entities.sort()\n",
    "\n",
    "    for item in entities:\n",
    "        print(item)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return\n",
    "\n",
    "# Use spacytextblob for sentiment analysis\n",
    "def sentiment_analysis(doc):\n",
    "    print(\"\\n\")\n",
    "    print(\"Polarity: \" + str(doc._.blob.polarity) + \"\\n\")\n",
    "    print(\"Subjectivity: \" + str(doc._.blob.subjectivity) +\" \\n\")\n",
    "\n",
    "    return\n",
    "\n",
    "# Return the top 5 most similar sentences to the user input\n",
    "def top_5_sentences(doc):\n",
    "\n",
    "    user_input = nlp(input(\"\\nEnter a sentence to compare with the document: \"))\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Compare the meaning of the user input with the sentences in the document\n",
    "    for sentence in doc.sents:\n",
    "\n",
    "        similarity_score = sentence.similarity(user_input)\n",
    "\n",
    "        results.append((similarity_score, sentence))\n",
    "\n",
    "    # Sort the sentences by similarity score\n",
    "    results.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Print the top 5 most similar sentences\n",
    "    for i in range(5):\n",
    "        print(\"\\n***** Entry \" + str(i) + \" ***** score: \" + str(results[i][0]) + \" *****\\n\")\n",
    "        print(results[i][1])\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle printing and calling the functions based on user input\n",
    "while(True):\n",
    "\n",
    "    print(\"***************\\n\")\n",
    "\n",
    "    print(\"Available operations:\\n\")\n",
    "    print(\"1. Extract people from document\\n\")\n",
    "    print(\"2. Extract problems/symptoms from document\\n\")\n",
    "    print(\"3. Extract tests from document\\n\")\n",
    "    print(\"4. Extract treatments from document\\n\")\n",
    "    print(\"5. Perform sentiment analysis on document\\n\")\n",
    "    print(\"6. List the top 5 sentences most similar to input\\n\")\n",
    "    print(\"7. Filter data based on disease or date\\n\")\n",
    "    print(\"8. Exit\\n\")\n",
    "\n",
    "    print(\"***************\\n\")\n",
    "\n",
    "    operation = input(\"Select operation to perform: \")\n",
    "\n",
    "    try:\n",
    "\n",
    "        operation = int(operation)\n",
    "\n",
    "    except:\n",
    "\n",
    "        print(\"Invalid input, try again \\n\")\n",
    "\n",
    "        continue\n",
    "\n",
    "    entity_types = ['person', 'problem', 'test', 'treatment']\n",
    "\n",
    "    if operation >= 1 and operation <= 4:\n",
    "        extract_entities(doc, entity_types[operation - 1])\n",
    "\n",
    "    elif operation == 5:\n",
    "        sentiment_analysis(doc)\n",
    "\n",
    "    elif operation == 6:\n",
    "        top_5_sentences(doc)\n",
    "\n",
    "    elif operation == 7:\n",
    "    \n",
    "        pass\n",
    "\n",
    "    elif operation == 8:\n",
    "\n",
    "        exit(0)\n",
    "\n",
    "    run_again = input(\"Run again? (y/n)\\n\")\n",
    "\n",
    "    if run_again == \"n\":\n",
    "        \n",
    "        exit(0)\n",
    "\n",
    "    else:\n",
    "\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter for use of command line tools\n",
    "!jupyter nbconvert --to script prototype5.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.17 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c423fc4b83a443d0088c583e30019265b651f290d59d26577f33384cc388e91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
